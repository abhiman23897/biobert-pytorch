{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"allenai/longformer-base-4096\"\n",
    "model = \"dmis-lab/biobert-base-cased-v1.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('Stardrums/pico-breast-cancer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = dataset[\"train\"].features[\"ner_tags\"].feature.names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bert expects input in `input_ids`, `token_type_ids` and `attention_mask` format\n",
    "- The label also requires adjustment due to subword tokenization used by BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see why we need to adjust the labels\n",
    "\n",
    "- We will process the tokens using tokenizer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"tokens\"], padding=\"max_length\", truncation=True, is_split_into_words=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets_ = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_['train'][0]['input_ids'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets_['train'][0]['ner_tags'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_datasets_['train'][0]['input_ids']) == len(tokenized_datasets_['train'][0]['ner_tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that len of `input_ids` is not matching with `ner_tags` that's why we require to adjust the labels according to the tokenized output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will use the argument truncation=True (to truncate texts that are bigger than the maximum size allowed by the model) as there is a sequence in data which has length>512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the values for input_ids, attention_mask, adjusted labels\n",
    "def tokenize_adjust_labels(all_samples_per_split):\n",
    "  tokenized_samples = tokenizer.batch_encode_plus(all_samples_per_split[\"tokens\"], is_split_into_words=True, truncation=True, max_length=512)\n",
    "  \n",
    "  total_adjusted_labels = []\n",
    "  \n",
    "  for k in range(0, len(tokenized_samples[\"input_ids\"])):\n",
    "    prev_wid = -1\n",
    "    word_ids_list = tokenized_samples.word_ids(batch_index=k)\n",
    "    existing_label_ids = all_samples_per_split[\"ner_tags\"][k]\n",
    "    i = -1\n",
    "    adjusted_label_ids = []\n",
    "   \n",
    "    for word_idx in word_ids_list:\n",
    "      # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "      # ignored in the loss function.\n",
    "      if(word_idx is None):\n",
    "        adjusted_label_ids.append(-100)\n",
    "      elif(word_idx!=prev_wid):\n",
    "        i = i + 1\n",
    "        adjusted_label_ids.append(existing_label_ids[i])\n",
    "        prev_wid = word_idx\n",
    "      else:\n",
    "        label_name = label_names[existing_label_ids[i]]\n",
    "        adjusted_label_ids.append(existing_label_ids[i])\n",
    "        \n",
    "    total_adjusted_labels.append(adjusted_label_ids)\n",
    "  \n",
    "  #add adjusted labels to the tokenized samples\n",
    "  tokenized_samples[\"labels\"] = total_adjusted_labels\n",
    "  return tokenized_samples\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_adjust_labels, batched=True, remove_columns=['tokens', 'ner_tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will now have all the required fields for training, 'input_ids', 'token_type_ids', 'attention_mask', 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset['train'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, different sample have different length therefore we need to \n",
    "pad the tokens to have same length "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://huggingface.co/docs/transformers/main/main_classes/data_collator#transformers.DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForTokenClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if gpu is present\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will use Distillbert-base-uncased model for fine tuning\n",
    "- We need to specify the number of labels present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model, num_labels=len(label_names))\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a function to generate metrics\n",
    "- We will use `seqeval` metrics, commonly used for token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seqeval -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "\n",
    "    #select predicted index with maximum logit for each token\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[\"train\"][1]\n",
    "labels = [label_names[i] for i in example[f\"ner_tags\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fine Tuning using Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "batch_size = 16\n",
    "logging_steps = 1#len(tokenized_dataset['train']) // batch_size\n",
    "epochs = 20\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/\",\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=logging_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine tune using train method\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the precision/recall/f1 computed for each category for test set, we can apply the same function as before on the result of the `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_dataset[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "- f1 score for LOC and PER is >85% and ORG has <75%\n",
    "- Overall f1 score is ~83%\n",
    "- We can improve the accuracy by training the model for more number of epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"token-classification\", model=trainer.model, tokenizer = tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.ensure_tensor_on_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Objectives: Primary objectives: to compare radial artery occlusion rate (RAO) after cardiac catheterization between catecholamine-chitosan pad (InnoSEAL) and pneumatic compression device (PCD) and to compare difference in hemostasis time and radial monitoring termination time between two arms. Secondary objectives: to compare radial site bleeding and ease of use of two methods by cath-lab technicians. Background: Hemostatic pads may be an effective alternative to PCD with lesser chance of access site complications with advantage of shortened compression time. Methods: Patients (N = 606) undergoing trans-radial, diagnostic or interventional procedures were randomized to either InnoSEAL arm or PCD. RAO was assessed using US Duplex; performed 6-24 hr posthemostatic device removal. Time to hemostasis was recorded as per defined protocols. Ease of use among cath-lab technicians was assessed through 5 point Likert scale. Results: Data of 597 patients was analyzed (299 InnoSEAL, 298 PCD). RAO rate was 8.5% in InnoSEAL and 9.4% in PCD arm (p value >.05). The pooled median hemostasis time and time to termination of radial monitoring was 42 versus 225 min and 50 versus 240 min in InnoSEAL and PCD arms, respectively (p value: <.01). There was no difference in Grade I/II hematoma (InnoSEAL: 1.3% vs. PCD: 3.4%). InnoSEAL was marginally acceptable compared to PCD by technicians. Conclusion: Hemostasis time is significantly shorter in InnoSEAL arm with reasonable acceptability to its usage among cath lab staff. RAO and bleeding complications are comparable between the arms. Based on our findings, it seems feasible to include Chitosan based hemostasis pad routinely in cath lab.\"\n",
    "\n",
    "print(pipe.predict(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abhiman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
